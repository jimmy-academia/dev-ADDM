The core idea is to support Natural Language Query Input; 
So we need phase 1 to use Natural Language Query and not Policy IR.


Don’t fix failures by tweaking prompt wording or adding benchmark-specific hints (e.g., “be extra careful about secondhand allergy cases” or “remember the threshold is usually 2”), because that is behavioral overfitting. Do fix failures by tightening the specification (e.g., require account_type ∈ {firsthand, secondhand, hypothetical}, validate evidence IDs against the input, and compute rule thresholds deterministically in code). Don’t rely on the model to “usually follow the rules.” Do enforce constraints so invalid outputs are rejected or abstained, making errors explicit and measurable.


