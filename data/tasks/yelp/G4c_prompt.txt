============================================================
Agenda
============================================================

Help assess the quality of server/staff service at this restaurant.

Analyze each review to identify mentions of server interactions and service quality.
For each record, extract the L0 primitives defined below.

This task uses complex formula with weighted service factors and interaction effects.

============================================================
L0: Primitives (per-review extraction)
============================================================

ATTENTIVENESS
- excellent: proactively attentive, anticipated needs
- good: responsive when needed
- adequate: acceptable but not memorable
- poor: had to flag down server repeatedly
- neglectful: felt ignored or abandoned
- not_mentioned: attentiveness not discussed

MENU_KNOWLEDGE
- expert: deep knowledge, great recommendations
- good: answered questions well
- limited: couldn't answer some questions
- poor: gave wrong information
- not_mentioned: knowledge not discussed

FRIENDLINESS
- warm: genuinely friendly, made guests feel welcome
- professional: polite and courteous
- neutral: neither friendly nor unfriendly
- cold: unfriendly or unwelcoming
- rude: actively rude or hostile
- not_mentioned: friendliness not discussed

ERROR_HANDLING
- excellent: fixed mistakes gracefully, compensated
- good: acknowledged and corrected errors
- poor: defensive or dismissive about errors
- denied: refused to acknowledge mistakes
- no_errors: no errors to handle
- not_mentioned: not discussed

PROFESSIONALISM
- exemplary: went above and beyond
- professional: met expectations
- unprofessional: inappropriate behavior noted
- not_mentioned: not discussed

PERSONALIZATION
- personalized: remembered preferences, made personal connection
- standard: normal service interaction
- impersonal: felt like just another customer
- not_mentioned: not discussed

SPECIFIC_SERVER_NAMED
- positive: server named with praise
- negative: server named with complaint
- not_named: no specific server mentioned

============================================================
L1: Composites (derived per review) - COMPLEX
============================================================

Attentiveness Score:
  excellent = +3.0
  good = +1.5
  adequate = 0
  poor = -2.0
  neglectful = -4.0
  not_mentioned = 0

Knowledge Score:
  expert = +2.0
  good = +1.0
  limited = -0.5
  poor = -2.0
  not_mentioned = 0

Friendliness Score:
  warm = +2.5
  professional = +1.5
  neutral = 0
  cold = -2.0
  rude = -4.0
  not_mentioned = 0

Error Handling Score:
  excellent = +2.5
  good = +1.0
  poor = -2.0
  denied = -3.5
  no_errors = +0.5
  not_mentioned = 0

Professionalism Score:
  exemplary = +2.0
  professional = +1.0
  unprofessional = -3.0
  not_mentioned = 0

Personalization Modifier:
  personalized = +1.5
  standard = 0
  impersonal = -0.5
  not_mentioned = 0

L1_SERVICE_SCORE = ATTENTIVENESS_SCORE + KNOWLEDGE_SCORE + FRIENDLINESS_SCORE + ERROR_HANDLING_SCORE + PROFESSIONALISM_SCORE + PERSONALIZATION_MODIFIER

RUDE_AND_NEGLECTFUL (interaction penalty):
  If FRIENDLINESS = rude AND ATTENTIVENESS in {poor, neglectful}: -3.0
  Else: 0

WARM_AND_KNOWLEDGEABLE (interaction bonus):
  If FRIENDLINESS = warm AND MENU_KNOWLEDGE = expert: +2.0
  Else: 0

NAMED_SERVER_WEIGHT:
  If SPECIFIC_SERVER_NAMED = positive: multiply final by 1.1
  If SPECIFIC_SERVER_NAMED = negative: multiply final by 1.2 (amplify complaints)
  Else: 1.0

L1_TOTAL_SCORE = (L1_SERVICE_SCORE + RUDE_AND_NEGLECTFUL + WARM_AND_KNOWLEDGEABLE) * NAMED_SERVER_WEIGHT

============================================================
L2: Aggregates (restaurant-level)
============================================================

Constants:
  BASE_SCORE = 5.0

Counts:
  N_SERVICE_REVIEWS = count of reviews with any service-related primitives
  N_RUDE = count where FRIENDLINESS = rude
  N_NEGLECTFUL = count where ATTENTIVENESS = neglectful
  N_WARM = count where FRIENDLINESS = warm
  N_EXCELLENT_ATTENTION = count where ATTENTIVENESS = excellent
  N_NAMED_POSITIVE = count where SPECIFIC_SERVER_NAMED = positive
  N_NAMED_NEGATIVE = count where SPECIFIC_SERVER_NAMED = negative

Confidence Level:
  if N_SERVICE_REVIEWS == 0: none
  elif N_SERVICE_REVIEWS <= 2: low
  elif N_SERVICE_REVIEWS <= 5: medium
  else: high

Aggregation:
  SUM_L1_SCORE = sum of L1_TOTAL_SCORE across all service reviews
  MEAN_L1_SCORE = SUM_L1_SCORE / max(N_SERVICE_REVIEWS, 1)

============================================================
Formulas & Decision Policy
============================================================

Formulas:
  ADJUSTED_SCORE = MEAN_L1_SCORE
  RAW_SCORE = BASE_SCORE + ADJUSTED_SCORE
  FINAL_SCORE = clamp(RAW_SCORE, 0.0, 10.0)

Decision Thresholds on FINAL_SCORE:
  - >= 7.5 => Excellent Service
  - >= 5.5 AND < 7.5 => Good Service
  - >= 3.5 AND < 5.5 => Mixed Service
  - < 3.5 => Poor Service

Overrides:
  - If N_RUDE >= 2 => Poor Service
  - If N_NEGLECTFUL >= 2 AND N_RUDE >= 1 => Poor Service
  - If N_NAMED_NEGATIVE >= 2 => max Mixed Service (systemic issue)
  - If MEAN_L1_SCORE >= 3 AND N_WARM >= 2 => min Good Service
  - If MEAN_L1_SCORE < -2 => Poor Service

============================================================
Output Schema
============================================================

{
  L0_primitives: [ ... one record per service-related review ... ],
  L1_composites: [
    {
      review_id,
      attentiveness_score,
      knowledge_score,
      friendliness_score,
      error_handling_score,
      professionalism_score,
      personalization_modifier,
      l1_service_score,
      rude_and_neglectful,
      warm_and_knowledgeable,
      named_server_weight,
      l1_total_score
    },
    ...
  ],
  L2_aggregates: {
    N_SERVICE_REVIEWS,
    CONFIDENCE_LEVEL,
    N_RUDE, N_NEGLECTFUL,
    N_WARM, N_EXCELLENT_ATTENTION,
    N_NAMED_POSITIVE, N_NAMED_NEGATIVE,
    SUM_L1_SCORE,
    MEAN_L1_SCORE,
    BASE_SCORE,
    ADJUSTED_SCORE,
    RAW_SCORE,
    FINAL_SCORE
  },
  DecisionPolicy: {
    base_verdict_by_score,
    override_applied,
    VERDICT
  },
  decision_evidence: {
    high_score_reviews: [ {review_id, l1_total_score, quote}, ... ],
    low_score_reviews: [ {review_id, l1_total_score, quote}, ... ],
    named_server_mentions: [ {review_id, sentiment, quote}, ... ]
  }
}
